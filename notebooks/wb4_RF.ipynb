{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting your google drive on Colab Noetebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remplazá 'unqml'\n",
    "# por el nombre que le hayas puesto a esa carpeta en drive si usaste otro distinto\n",
    "%cd drive/My Drive/unqml/ML_UNQ2020/ \n",
    "datapath = './data/VariantEffectPrediction/clinvarHC_modeling.csv.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utility_functions as uf   ## cargo funciones\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "## learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "### model performance\n",
    "from sklearn import metrics\n",
    "\n",
    "#ploting modules\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "## además cargo funciones específicas para este notebook\n",
    "try:\n",
    "    import category_encoders as ce\n",
    "except:\n",
    "    !pip install category_encoders\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Para guardar y levantar un objeto en pickle (ej. resultados de un gridsearch intensivo)\n",
    "import joblib as jlb\n",
    "\n",
    "\n",
    "## Hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except:\n",
    "    !pip install shap\n",
    "    import shap\n",
    "\n",
    "\n",
    "try:\n",
    "    import rfpimp\n",
    "except:\n",
    "    !pip install rfpimp\n",
    "    import rfpimp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and define preprocessing\n",
    "SEED = 2718281\n",
    "targetcolumn = 'ClinvarHC'\n",
    "\n",
    "data = pd.read_csv(datapath,sep = ',',index_col='ChrPosRefAlt')\n",
    "X,y = data.drop([targetcolumn],axis = 1), data[[targetcolumn]]\n",
    "\n",
    "## train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=SEED,stratify = y)\n",
    "Y_train = y_train[targetcolumn].ravel()\n",
    "Y_test = y_test[targetcolumn].ravel()\n",
    "\n",
    "categorical_feature_mask = X_train.dtypes==object       # esto nos da un vector booleano \n",
    "categorical_columns = X_train.columns[categorical_feature_mask].tolist()  # acá picnhamos los nombres de esas columnas\n",
    "numerical_columns = X_train.columns[~X_train.columns.isin(categorical_columns)].tolist() # defino las numéricas como el complemento de las categóricas \n",
    "\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=False))])  # Esto es una vacancia de Sklearn, no permite aún \"centrar\" matrizes sparse\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')),  \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])      \n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('num', numerical_transformer,numerical_columns),\n",
    "    ('cat', categorical_transformer,categorical_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('preprocessing',preprocessor),\n",
    "                             ('clasificador',RandomForestClassifier(random_state = SEED,n_estimators = 50))])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv = cross_val_score(pipeline, X_train, Y_train, cv=5,scoring = 'average_precision')\n",
    "print(np.mean(rfcv),np.std(rfcv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The wisdom of crowds\n",
    "#### Sin tunear ningún hiperparámetro, ya estamos arriba de la LR y el DT que hemos tuneado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuevos hiperparámetros en adición a un simple Decision Tree:\n",
    "\n",
    "***n_estimators*** : número de árboles a utilizar \n",
    "\n",
    "***n_jobs :***  RF es fácilmente paralelizable (cada estimador es independiente de los otros)\n",
    "\n",
    "\n",
    "***max_features***: número de features que usa cada árbol. Notar que aunque el número es igual para todos, ahora cada árbol usa un set de N features distintos!\n",
    "\\### Este ya estaba, en Decision Trees, pero acá toma particuar importancia. De hecho es lo que diferencia conceptuamente a RF de un método convencional de bagging (promediar resultados de clasificadores), que cada árbol no emplea todos los features, usa un subset reducido (distinto de los demás!)  \n",
    "\n",
    "***oob_score :*** out of bag error. Nos dice para cada feature, cómo cae en promedio el desempeño del modelo en todos los árboles (estimadores) que no lo incluyeron en la selección de features. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorarción de hiperparámetros\n",
    "Ver:\n",
    "\n",
    "__[Acá, exploración individual](https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numero de estimadores (árboles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestimators = np.array([10, 20 , 50, 100, 500])\n",
    "\n",
    "param_grid = [\n",
    "  {'clasificador__n_estimators':nestimators}\n",
    " ]\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=3,return_train_score = True,\n",
    "                     scoring = 'roc_auc').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'n_estimators'\n",
    "value_range = [10,50,100,200,500]\n",
    "\n",
    "## defino grilla\n",
    "gridname = 'clasificador__%s'%name\n",
    "param_grid = [\n",
    "  {gridname:value_range}\n",
    " ]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=5,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [7, 4]\n",
    "uf.my_train_test_plot(gridsearch=search,grid=param_grid,hyp=name,ylim = [0.25,1.05], ylabel= 'avg_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noten que, el train está sobreestimado al máximo y que aumentar el número de estimadores está achicando la brecha de varianza. Es decir, está generalizando mejor.  \n",
    "* Esa sobreestimación del training hay que pensarla relacionada al seteo por default de los demás hiperparámetros.\n",
    "* Arboles muy profundos? \n",
    "* Hojas de muy pocos elementos? \n",
    "* Estamos admintiendo splits muy chiquitos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'max_features'\n",
    "value_range = np.linspace(0.01, 1, 10, endpoint=True)\n",
    "\n",
    "## defino grilla\n",
    "gridname = 'clasificador__%s'%name\n",
    "param_grid = [\n",
    "  {gridname:value_range}\n",
    " ]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=5,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [7, 4]\n",
    "uf.my_train_test_plot(gridsearch=search,grid=param_grid,hyp=name,ylim = [0.25,1.05], ylabel= 'avg_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Samnples Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'min_samples_split'\n",
    "value_range =  np.linspace(0.01, 0.1, 10, endpoint=True)\n",
    "value_range = [2,0.001,0.005] +[m for m in value_range]+ [0.15,0.2,0.3,0.4,0.5,0.6,0.65,0.7,1.0]\n",
    "\n",
    "## defino grilla\n",
    "gridname = 'clasificador__%s'%name\n",
    "param_grid = [\n",
    "  {gridname:value_range}\n",
    " ]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=5,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [7, 4]\n",
    "uf.my_train_test_plot(gridsearch=search,grid=param_grid,hyp=name,ylim = [0.25,1.05], ylabel= 'avg_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acá frenemos. \n",
    "#### Miremos el gráifco y pensemos/ contrastemos con lo que le pasaba a un árbol individual. \n",
    "* No cae abrupto el score en el evaluation set cuando reducimos al mínimo el valor de min_samples split.\n",
    "* Esto es gracias al bagging. Aunque un arbol en particular sobreajuste por permitirle splitear en hojas de muy pocos elementos, el promedio a lo largo de todos los árboles controla el sobreajuste. En pocas palabras, controla complegidad del modelo. \n",
    "\n",
    "\n",
    "\n",
    "* De los parámetros que movimos, es el primero hasta ahora que vemos controlar el sobreajuste en el training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volvamos a explorar, por ejemplo \n",
    "#### Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps['clasificador'].set_params(**{'min_samples_split':0.50})\n",
    "pipeline.named_steps['clasificador']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps['clasificador'].set_params(**{'min_samples_split':0.05})\n",
    "name = 'max_features'\n",
    "value_range = np.linspace(0.001, 0.5, 20, endpoint=True)\n",
    "\n",
    "## defino grilla\n",
    "gridname = 'clasificador__%s'%name\n",
    "param_grid = [\n",
    "  {gridname:value_range}\n",
    " ]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=5,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [7, 4]\n",
    "uf.my_train_test_plot(gridsearch=search,grid=param_grid,hyp=name,ylim = [0.25,1.05], ylabel= 'avg_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máxima profundiad del arbol admitida (Max Depth)\n",
    "Otro que controla complegidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'max_depth'\n",
    "\n",
    "value_range = np.linspace(1, 40, 20, endpoint=True)\n",
    "value_range = [int(m) for m in value_range]  # esto no es necesario, sólo está acá por cómo estamos ploteando\n",
    "\n",
    "\n",
    "\n",
    "## defino grilla\n",
    "gridname = 'clasificador__%s'%name\n",
    "param_grid = [\n",
    "  {gridname:value_range}\n",
    " ]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=5,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [7, 4]\n",
    "uf.my_train_test_plot(gridsearch=search,grid=param_grid,hyp=name,ylim = [0.25,1.05], ylabel= 'avg_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimal Impurity of leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'min_impurity_decrease'\n",
    "\n",
    "value_range  = np.linspace(0,0.02, 20, endpoint=True)\n",
    "\n",
    "## defino grilla\n",
    "gridname = 'clasificador__%s'%name\n",
    "param_grid = [\n",
    "  {gridname:value_range}\n",
    " ]\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=5,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [7, 4]\n",
    "uf.my_train_test_plot(gridsearch=search,grid=param_grid,hyp=name,ylim = [0.25,1.05], ylabel= 'avg_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('preprocessing',preprocessor),\n",
    "                             ('clasificador',RandomForestClassifier(random_state = SEED, n_estimators=50))]) \n",
    "\n",
    "min_samples_splits = np.linspace(0.05, 0.2, 5, endpoint=True)\n",
    "max_depths = np.linspace(3, 9, 7, endpoint=True)\n",
    "max_features= np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "\n",
    "param_grid = [\n",
    "  {\n",
    "      'clasificador__min_samples_split':min_samples_splits,\n",
    "      'clasificador__max_depth':max_depths,\n",
    "      'clasificador__max_features':max_features\n",
    "#      'clasificador__min_impurity_decrease':min_impurity_decrease\n",
    "  }\n",
    " ]\n",
    "search = GridSearchCV(pipeline, param_grid, \n",
    "                      cv=10,return_train_score = True,\n",
    "                     scoring = 'average_precision').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('performance best classif %.4f'%search.best_score_)\n",
    "print('best hyperparameters: \\n')\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "aux= cross_val_score(search.best_estimator_, X_train, Y_train, cv=10,scoring = 'average_precision')\n",
    "print(np.mean(aux),np.std(aux))\n",
    "\n",
    "print(metrics.average_precision_score(Y_test,search.best_estimator_.predict_proba(X_test)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.average_precision_score(Y_test,search.best_estimator_.predict_proba(X_test)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv = cross_val_score(search.best_estimator_, X_train, Y_train, cv=10,scoring = 'average_precision')\n",
    "print(np.mean(rfcv),np.std(rfcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv = cross_val_score(pipeline, X_train, Y_train, cv=10,scoring = 'average_precision')\n",
    "print(np.mean(rfcv),np.std(rfcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = search.best_estimator_\n",
    "pipe2.named_steps['clasificador'].set_params(**{'n_estimators':500})\n",
    "rfcv = cross_val_score(pipe2, X_train, Y_train, cv=10,scoring = 'average_precision')\n",
    "print(np.mean(rfcv),np.std(rfcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('preprocessing',preprocessor),\n",
    "                             ('clasificador',RandomForestClassifier(random_state = SEED,n_estimators = 2000))])  \n",
    "\n",
    "rfcv = cross_val_score(pipeline, X_train, Y_train,  cv=10,scoring = 'average_precision')\n",
    "print(np.mean(rfcv),np.std(rfcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.average_precision_score(Y_test,pipeline.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les dejo este post con algo de info de F.I en RF más allá del default basado en Out of Bag\n",
    "\n",
    "__[Acá, algo de Feature Importance en RF](https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_names = pipeline.named_steps['preprocessing'].transformers[0][2]\n",
    "categorical_output_names = pipeline.named_steps['preprocessing'].named_transformers_['cat'].named_steps['onehot'].get_feature_names().tolist()\n",
    "\n",
    "cols = numerical_names + categorical_output_names\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 8]\n",
    "clf = pipeline.named_steps['clasificador']\n",
    "fi = pd.Series(clf.feature_importances_,index = cols )\n",
    "s = fi.sort_values(ascending =False)[0:20]\n",
    "s.sort_values(ascending =True).plot.barh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_train.corr()\n",
    "\n",
    "cg = sns.clustermap(corr.fillna(0), cmap ='coolwarm', linewidths = 0.2,\n",
    "                    method = 'complete', xticklabels=1,figsize = (17,17)) \n",
    "cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "corr_thr = 0.4\n",
    "ccols = corr.columns\n",
    "cluster = AgglomerativeClustering(n_clusters=None, affinity='precomputed',\n",
    "                                  linkage='complete',distance_threshold=1-corr_thr)\n",
    "etiquetas = cluster.fit_predict(1-abs(corr))\n",
    "cldf = pd.DataFrame({'feature':corr.columns,'clusterID':etiquetas})\n",
    "res = dict(zip(corr.columns, etiquetas)) \n",
    "\n",
    "feat =[]\n",
    "for e in np.unique(etiquetas):\n",
    "    i = etiquetas == e\n",
    "    f = ccols[i]\n",
    "    feat.append(list(f))\n",
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldf.sort_values(by='clusterID').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfpimp import permutation_importances, importances, plot_importances\n",
    "\n",
    "def avPrec(model, X, y,sample_weight=None):\n",
    "    return metrics.average_precision_score(y_true = y, y_score = model.predict(X),sample_weight =sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I0 =   importances(pipeline, X_train, y_train, metric = avPrec)\n",
    "#plot_importances(I0,width=25)\n",
    "I0.sort_values(by='Importance',ascending=True).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iclust = importances(pipeline, X_train, y_train, metric= avPrec,features=feat,n_samples=-1)\n",
    "plot_importances(Iclust,width=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos verlo en en test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iclust = importances(pipeline, X_test, y_test, metric= avPrec,features=feat,n_samples=-1)\n",
    "plot_importances(Iclust,width=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esto es de los tipos de rfpimp:\n",
    "\n",
    "(como lectura complementaria, y a absorber con espíritu crítico)\n",
    "\n",
    "__[Explained AI (de los creadores de rfpimp)](https://explained.ai/rf-importance/index.html#5)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "#import lime ### hay que verlo (e instalarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pipeline.named_steps['preprocessing']\n",
    "model = pipeline.named_steps['clasificador']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xprep = pp.transform(X_train)\n",
    "Xprep = pd.DataFrame(Xprep)  # preproceso para que SHAP pueda perturbar sin nulls\n",
    "\n",
    "# recupero nombre de columnas\n",
    "numerical_names = pp.transformers_[0][2]\n",
    "categorical_output_names = pp.named_transformers_['cat'].named_steps['onehot'].get_feature_names().tolist()\n",
    "cols = numerical_names + categorical_output_names\n",
    "Xprep.columns = cols\n",
    "\n",
    "Xtrain_sampled = shap.sample(Xprep,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_tree_explainer = shap.TreeExplainer(model, Xtrain_sampled,model_output = 'probability')\n",
    "shap_values = shap_tree_explainer.shap_values(Xtrain_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], Xtrain_sampled, plot_type=\"dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction plots \n",
    "* para entender valor predictivo conjunto de grupos de dos variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('loftool_loftool_score', shap_values[1], Xtrain_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('phylop_phylop100_vert', shap_values[1], Xtrain_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"rank(1)\",shap_values[1], Xtrain_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(ind = \"rank(2)\",shap_values = shap_values[1], \n",
    "                     features = Xtrain_sampled,interaction_index = \"gnomad_gene_oe_lof_min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(ind = 'phylop_phylop100_vert',shap_values = shap_values[1], \n",
    "                     features = Xtrain_sampled,interaction_index = 'phastcons_phastcons100_vert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicando predicciones localmente (a nivel de ítem individual) \n",
    "Ojo q es costoso en cálculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train[Xtrain_sampled.index].mean()\n",
    "Xtrain_sampled.mean().loc[['gnomad_gene_syn_z_max','gnomad_gene_oe_syn_min',\n",
    "                           'biogrid_counts','phylop_phylop100_vert'],]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localplot(j,tree_explainer=shap_tree_explainer,shapValues=shap_values,X=Xtrain_sampled):\n",
    "    return shap.force_plot(tree_explainer.expected_value[1], shapValues[1][j],X.iloc[[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_tree_explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=2\n",
    "print('original class: %s'%Y_train[Xtrain_sampled.index][j])\n",
    "\n",
    "localplot(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recordar que SHAP ES ADITIVO:\n",
    "* expected + sum(contributions) = model output value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el segundo ítem (j = 2)\n",
    "shap_tree_explainer.expected_value[1] + shap_values[1].sum(axis=1)[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (notar que ese valor, 0.08 coincide con el gráfico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veamos otro ejemplo, una variante patogénica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=3\n",
    "print('original class: %s'%Y_train[Xtrain_sampled.index][j])\n",
    "\n",
    "localplot(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de shap output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_values = shap_tree_explainer.expected_value[1] + shap_values[1].sum(axis=1)\n",
    "classified_as_possitive  = output_values - shap_tree_explainer.expected_value[1]\n",
    "\n",
    "pd.Series(shap_values[1].sum(axis = 1)).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_tree_explainer = shap.TreeExplainer(model, Xtrain_sampled, feature_perturbation = \"tree_path_dependent\",model_output = 'raw')\n",
    "#interaction_values = shap_tree_explainer.shap_interaction_values(Xtrain_sampled)[1]\n",
    "#shap.decision_plot(shap_tree_explainer.expected_value[1],\n",
    "#                   interaction_values[1,:,:],feature_names=cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
